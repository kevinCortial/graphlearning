{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinCortial/graphlearning/blob/main/classification_gnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> Graph Learning on Product Space </center></h1>\n",
        "\n",
        "<center> Kévin CORTIAL et Taoufik JARMOUNI &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; Open Studio UCA IP / Atlas des Synergies Productives &nbsp;&nbsp;&nbsp; </center>"
      ],
      "metadata": {
        "id": "CVEpCw6wXl3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "8CHxrKrCZiF3",
        "outputId": "060971be-36c4-45ec-f8df-91dfb40b1ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Installing collected packages: psutil, dgl\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-0.9.1 psutil-5.9.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dgllife"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFo0hcrb7cfx",
        "outputId": "66d793ec-04c9-4277-aeca-47ab8c21a22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgllife\n",
            "  Downloading dgllife-0.3.0-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.6.3)\n",
            "Collecting scikit-learn<1.0,>=0.22.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgllife) (4.64.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (1.21.6)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from dgllife) (0.1.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from dgllife) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->dgllife) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0,>=0.22.2->dgllife) (3.1.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (4.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->dgllife) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->dgllife) (2.8.2)\n",
            "Installing collected packages: scikit-learn, dgllife\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
            "Successfully installed dgllife-0.3.0 scikit-learn-0.24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#This cell allows us to grab our needed resources from Google Drive directly\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doR984a1aAE1",
        "outputId": "3869ce22-6bf1-4989-f5b6-c9c3ee3aafb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import dgl.data\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "g = dataset[0]\n",
        "\n",
        "print(g.ndata)\n",
        "print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Uo4Ap3cwAf",
        "outputId": "6a8d28a0-571c-41c4-e6cf-dab8ac390c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(1433,), dtype=torch.float32)}\n",
            "      edata_schemes={})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "g.subgraph() déjà essayé (par contre je ne suis pas sûr d'avoir tout essayé)\n",
        "\n",
        "nx.weakly_connected_components(g)\n",
        "\n",
        "dgl.data.utils.Subset()\n",
        "\n",
        "dgl.distributed.dist_graph.node_split()"
      ],
      "metadata": {
        "id": "hWBOjCzz-mGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import GraphConv\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from dgl.data.utils import split_dataset, Subset\n",
        "from dgllife.utils import ConsecutiveSplitter\n",
        "\n",
        "g = nx.read_gpickle(\"/content/drive/MyDrive/Open_Studio/product_graphWithData4int.gpickle\")\n",
        "#categorie = list(g.nodes(data=\"categorie_macro\"))"
      ],
      "metadata": {
        "id": "EbSTKLm68OVo",
        "outputId": "0bb742ab-baad-4e9e-dfd1-ef628a227664",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDKit is not installed, which is required for utils related to cheminformatics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNEOb3ZhXPn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6a92ae-f98e-4b17-8679-d9427447f7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<dgl.data.utils.Subset object at 0x7f0452f60650>, <dgl.data.utils.Subset object at 0x7f0452f60810>, <dgl.data.utils.Subset object at 0x7f0452f60990>]\n",
            "In epoch 0, (best 0.000), (best 0.000)\n",
            "In epoch 5, (best 0.000), (best 0.000)\n",
            "In epoch 10, (best 0.000), (best 0.000)\n",
            "In epoch 15, (best 0.000), (best 0.000)\n",
            "In epoch 20, (best 0.000), (best 0.000)\n",
            "In epoch 25, (best 0.000), (best 0.000)\n",
            "In epoch 30, (best 0.000), (best 0.000)\n",
            "In epoch 35, (best 0.000), (best 0.000)\n",
            "In epoch 40, (best 0.000), (best 0.000)\n",
            "In epoch 45, (best 0.000), (best 0.000)\n",
            "In epoch 50, (best 0.000), (best 0.000)\n",
            "In epoch 55, (best 0.000), (best 0.000)\n",
            "In epoch 60, (best 0.000), (best 0.000)\n",
            "In epoch 65, (best 0.000), (best 0.000)\n",
            "In epoch 70, (best 0.000), (best 0.000)\n",
            "In epoch 75, (best 0.000), (best 0.000)\n",
            "In epoch 80, (best 0.000), (best 0.000)\n",
            "In epoch 85, (best 0.000), (best 0.000)\n",
            "In epoch 90, (best 0.000), (best 0.000)\n",
            "In epoch 95, (best 0.000), (best 0.000)\n"
          ]
        }
      ],
      "source": [
        "from dgl.nn import GraphConv\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from dgl.data.utils import split_dataset, Subset\n",
        "from dgllife.utils import ConsecutiveSplitter\n",
        "\n",
        "g = nx.read_gpickle(\"/content/drive/MyDrive/Open_Studio/product_graphWithData4int.gpickle\")\n",
        "#categorie = list(g.nodes(data=\"categorie_macro\"))\n",
        "\n",
        "#print(categorie)\n",
        "#tpl_categorie = list(zip(*categorie))[1]\n",
        "#print(tpl_categorie)\n",
        "#lst_categorie = list(tpl_categorie)\n",
        "#print(lst_categorie)\n",
        "g = dgl.from_networkx(g, node_attrs=['data', 'categorie_macro'])\n",
        "\n",
        "test = ConsecutiveSplitter.train_val_test_split(dataset, frac_train=0.8, frac_val=0.1, frac_test=0.1)\n",
        "\n",
        "print(test)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata['data']\n",
        "    labels = g.ndata['categorie_macro']\n",
        "    #lst_int_categorie = [int(x) for x in lst_categorie]\n",
        "    #pt_tensor_from_list = torch.FloatTensor(lst_int_categorie)\n",
        "\n",
        "    #labels = pt_tensor_from_list\n",
        "    #liste pytorch des ID catégorie\n",
        "    '''\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    print(train_mask)\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    '''\n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        '''\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "        '''\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        '''\n",
        "        loss.backward()'''\n",
        "        optimizer.step()\n",
        "\n",
        "        '''\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))'''\n",
        "            \n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, (best {:.3f}), (best {:.3f})'.format(\n",
        "                e, best_val_acc, best_test_acc))\n",
        "            \n",
        "\n",
        "# Create the model with given dimensions\n",
        "model = GCN(g.ndata['data'].shape[1], 16, dataset.num_classes)\n",
        "train(g, model)"
      ]
    }
  ]
}